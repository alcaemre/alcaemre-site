{"/":{"title":"alcaemre.com","content":"\nHello and welcome to Emre Alca's site! \n\nIf you are new here, I (Emre) would highly recommend you read my [[articles/mission-statement|mission statement]] so that you can have a decent idea of what I am about. To understand my approach to writing I would direct you towards my other meta-articles like [[articles/on-humility|On Humility]]. \n\n## Projects:\n### Doing vs. Being\nIn Cognitive Science at least, [[jargon/Functionalism|Functionalism]] has been the stance used to define minds for a long time--that to be a mind is to be capable of what minds do. Dretske says that mind must have the capacity to represent information, but also misrepresent information. Others say that a systesm must be able to attribute mental states, or perform some judgement of relevance realization (i.e. the intelligent ignoring of the vast majority of information) to be a mind. \n\n[Aman Bhargava](https://aman-bhargava.com/) and I have been considering another notion: what if functionalism is looking at the problem at the wrong level. Most minds that are discussed--be that human minds, animal minds, or the collective mind of an ant colony (an example I will often use)--[[jargon/emergence|emerge]] from the interactions between large populations of discrete parts. \n\nThe central intuition of this project is that any two systems that exhibit the same emergent behavior are comprised of parts that interact in ways that are described by analagous rules. That it is not the mental states that must have a functional lens put on them, but the neuron, or the ant.\n\nThe purpose of this project is to develop this stance, pulling from Philosophy of Mind, Mathematics and Computation, Shannon Information, Thermodynamics and Statistical Physics. \n\nAman and I believe that this notion holds true not only for the creation of minds, but the engineering of any [[jargon/Complex System|complex system]]. \n\u003e Articles:\n\u003e 1. [[articles/dvb-thesis|Doing vs. Being -- Understanding the Thesis]] (October 19, 2022). Introducing the idea of the [[jargon/homomorphism|homomorphism]] between emergent systems. The same patterns are present in seemingly very distinct organisms. Could the processes that make these similar patterns be analogous to one another?\n\u003e 2. [[articles/What You Must Keep, What You May Discard|What You Must Keep, What You May Discard]] (October 28, 2022). Philosophical discussion in the cognitive sciences about the level of emergence to go about analyzing cognition. Specifically, this discussion builds up to the idea of a multileveled ontology. This paper was an assignment for PHL342: *Minds and Machines* at the University of Toronto.\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/articles/What-You-Must-Keep-What-You-May-Discard":{"title":"What You Must Keep, What You May Discard","content":"\n## Introduction\n\nDeveloping a theory, of any kind, in any field, is a non-trivial task. Developing a theory of mind is the most abstract of theoretical realms. I think that the abstractness of the mind demands a certain level of theoretical care. Physicists talk about a theory as a *[[jargon/homomorphism|homomorphism]]*--as a unidirectional map from the real world to a model one. In this model world, all the relevant relationships are maintained, and all other information is collapsed into a *kernel*. Some physicists say that the real work of being a theorist lies in the creation of the kernel. The larger your kernel is, the more elegant the theory, but also the more likely that some important relationship is missing. If the kernel is too large, the theory is clumsy and inefficient. Many physicists say that the true skill of a theorist is the ability to discern what you must keep and what you may discard. \n\nThis paper is concerned with the question \"is it possible for a non-human machine to believe the very same thing as you (i.e. to have a belief that has the same intentional content as one of your beliefs)?\" Specifically, the perspective of the [[jargon/Functionalism|Functionalists]], the Eliminative Materialists, and later in the paper, Daniel Dennett (who is particularly difficult to label--and probably quite proud of it). I believe the disagreements between these stances arises from their different kernels.\n\n## The Functionalist Stance\nThe Functionalists (with a capital 'F' as I may yet be a functionalist, but I know I am not a Functionalist) are interested in a computational metaphor for cognition. I think Fodor is quite clear himself when he says:\n\n\u003e\"An important tendency in cognitive science is to treat the mind chiefly as a device that manipulates symbols. If a mental process can be functionally defined as an operation on symbols, there is a Turing machine capable of carrying out the computation and a variety of mechanisms for realizing the Turing machine. (Fodor, 120)\"\n\nThis shows Fodor's true mission. The mental states can have intentional content or qualitative content. Qualitative content encompasses feelings (often referred to as 'what it is like'-ness) associated with an experience. Intentional content is propositional content--something that can be assigned a truth-value. Fodor is not concerned with qualitative content. He believes there are no causal relationships between the qualitative content of mental states. On the other hand, beliefs with intentional content have clear causal relationships. They have implications, they can build into larger arguments, they are the foundation of formal logic. Fodor thinks that the only structure we need to be interested in is the one that describes the relationships between the intentional content of beliefs--in the language of thought.\n\nDretske is a Functionalist who treats the argument with a little more sensitivity. He discusses what it means for a system to have intentional beliefs, and especially what it means for a system to have intentional beliefs of its own. Dretske uses the example of a compass. He says that a compass always represents some propositional content, even when there is no person looking at it. \"The intentional states a compass occupies do not depend on our explanatory purposes, attitudes, or stances\" (Dretske, 471). Even when the compass is stowed away in a pocket, it is still pointing north. To build up to the level of a system that not only represents intentional content, but can create representations with the same kind of intentional content that our representations have, Dretske builds up to a more thorough recipe:\n\n\u003e \"Our recipe yields a product having the following properties:\n\t\t\t1. The product has a propositional content that represents the world in an aspectual way (as, say, $F$ rather than $G$ even when $F$s are always $G$).\n\t\t\t2. This content can be either true or false.\n\t\t\t3. The product is a “player” in the determination of system output (thus helping to explain system behavior).\n\t\t\t4. The propositional content of this product is the property that explains the product’s role in determining system output. The system not only does what it does because it has this product, but what it is about this product that explains why the system does what it does is its propositional content.\n\t\t\t5. Though the system can behave stupidly, the normal role of this product (the role it will play when it is doing the job for which it was created) will be in the production of intelligent (need and desire satisfaction) behavior.\n\tThis, it seems to me, is about all one could ask of a naturalistic recipe for thought.\" (Dretske, 481)\n\nParts 1. and 2. are building up that this 'product' must be able to represent intentional content according to its most specific aspect (e.g. jersey cows are a category even though all jersey cows are cows). Part 3. speaks to how a system must have some natural function which informs its structure (e.g. the structure of the heart is informed by its function of pumping blood). Part 4. speaks to how the system must be able to genuinely misrepresent things in the world (e.g. an altimeter put in a vacuum chamber will think it is very high in the atmosphere, even if it is at sea level). Finally, 5. posits that a mental state must be able to interact with other mental states--to be capable of that Fodorian symbol operation. I think that Dretske has laid out a very clear account of what it is for a belief to have intentional content, but I do not see how these parts can be put together into something that has the kind of beliefs that humans do.\n\n## Two Bushes Trimmed to the Shape of Two Elephants\nIn Dennett's *Real Patterns*, there is a quote from Quine, that is quite tragically left to a footnote. Quine says:\n\n\u003e\"Different persons growing up in the same language are like different bushes trimmed and trained to take the shape of identical elephants. The anatomical details of twigs and branches will fulfill the elephantine form differently from bush to bush, but the overall outward results are the same.\" (Quine, 1960, 8)\n\nThis quote sums up my problem with both functionalism and Eliminative materialism. To look only at the shape of the bush from the outside misses the very important internal structure of branches and roots. We are not Laplace's demon, we cannot look only at the structure of the branches and roots and determine the overall shape. The kernel is too small or too big. The Eliminative Materialists keep too much, and the Functionalists keep too little.\n\nDennett thinks we need to be somewhere closer to the middle of this debate. He believes that mental states cannot be dispensed of, because we are looking to make a system that has mental states. Dennett proposes theory on the level of *intentions* (as in the seeking of goals, not the having of propositional content) as a reliable way of predicting behavior. This is because it allows us to detect the important relationships in the most efficient way--to develop a kind of 'folk psychology'. The intentional stance keeps what is necessary and discards the rest. \nDennett is also quite careful to say that while this is the right level to talk about the mind--to notice the interesting patters, it is not the level where the thoughts are happening. He says:\n\n\u003e\"The process that produces the data of folk psychology, we claim, is one in which the multidimensional complexities of the underlying processes are projected *through linguistic behavior*, which creates an appearance of definiteness and precision, thanks to the discreteness of words\" (Dennett, 45)\n\nDennett is more sympathetic to the idea that mental states are not explicitly things in the head, but something closer to \"indirect 'measurements' of a reality diffused in the behavioral dispositions of the brain (and body)\" (Dennett, 45). This is an idea that is originally from Churchland, and Dennett co-opts it quite carefully--he maintains that while the interesting action is not happening on the level of some 'language of thought', there is some pattern above the level of just sets of neurons firing. This is what he means when he says \"If the \"pattern\" is scarcely an improvement over the bit map, talk of eliminative materialism will fall on deaf ears—just as it does when radical eliminativists urge us to abandon our ontological commitments to tables and chairs.\" (Dennett, 51). \n\n## How Do We Find the Right Level For Analysis?\n\nI am quite sympathetic to Dennett's definition of prescription: a system has mental states when it can be best and most robustly described from the intentional stance--when it can be attributed beliefs and desires that reliably predict its behavior. But I'm still not quite satisfied. I am a kind of functionalist, I do believe that a non-human machine can have the same kind of mental states that we do (and not just beliefs with intentional content, but qualitative content as well). Animals are the first example--they clearly can have mental states even though they are non-human.\n\n![murmuration of starlings](images/7uUb.gif)\n\u003e Fig.1 A murmuration of starlings\n\nBefore returning to the discussion to the level of minds, I would like to point at a murmuration of starlings. I believe that if we make little robots that have similar physical constraints and similar goals (be that differential attractors or some other mechanism) to real starlings, they will, in some important sense, become starlings--and thus will make murmurations of their own. We do not need to know all the mechanics of all the birds, just the rules that describe their interactions with one another. When working with these [[jargon/emergence|emergent patters]], identifying the right level of interaction is very nontrivial. For the mind, the level of the cell, or the signal seems quite natural to me, but it still is at great risk of falling prey to Churchland's issue of not doing enough compression. And still, looking at too high of a level, something like the level of propositional attitudes seems like too great a leap. Dennett's notion of a multileveled ontology, to make changes at the low level to look at outcomes at the high level, seems quite promising to me.\n\n## References\n1. Churchland, PM. (1981). Eliminative Materialism And The Propositional Attitudes. The Journal of Philosophy.\n2. Dennett, DC. (1991). Real Patterns. The Journal of Philosophy.  \n3. Dretske, F. (1994). If You Can’t Make One, You Don’t Know How It Works. Midwest Studies in Philosophy.  \n4. Fodor, J. (1980). The Mind-Body Problem. Scientific American. \n5. Quine, WVO. (1960). Word and Object. MIT Press.","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/articles/dvb-thesis":{"title":"Doing vs. Being -- Understanding the Thesis","content":"\n## What does *Doing vs. Being* Claim?\n\nThe purpose of this article is to understand what *Doing vs. Being* is claiming. Many of the details of this argument will have their own articles, where their nuances will be thoroughly explored.\n\nThe central claim of *Doing vs. Being* is that *if two systems display the same [[jargon/emergence|emergent]] behavior, they have a [[jargon/homomorphism|homomorphic relationship]].* This is a very non-trivial claim, which makes it difficult for me to call it a hypothesis or a theory. For now, it is just an intuition. I aim to explore its validity together with you, the reader, as it develops in these articles.\n\n## What is a Homomorphism?\nThe concept of a homomorphism was introduced to me by my nonlinear dynamics instructor. This was less of a technical introduction, but rather to motivate what it means to do physics. \nIf a physicist want to study some parts of the world, they must create some theory where the relationship between those parts are the same in the theory as they are in the world. The homomorphism is the structure preserving function that maps from theory to reality.\n\n![homomorphism figure](images/homomorphism.svg)\n\u003e Fig. 1: Consider a homomorphism $h$, which maps from $G$ to $H$. $h$ maps all possible $aN$ into $\\text{im}h$ (the image of $h$), maintaining the relationships between all $a$ in $aN$. All other parts of $G$ are collapsed into the *kernel* $(N=\\text{ker}h)$. [Image Source](https://en.wikipedia.org/wiki/Group_homomorphism) \n\nA very important feature of homomorphism in the kernel. The kernel contains all parts of reality that do not affect the relationships with which we are concerned. The real skill of a physicist, according to this particular instructor, is deciding what you must retain and what you may discard.\n\n## Homomorphic Relationships Between Physical Systems\n\nHomomorphic relationships do not only exist as maps between theory and reality. Seemingly very different physical systems can have a homomorphic relationship--they very often do. One of the first physics demonstrations I saw when I was young was the use of some weights and stretchy fabric to emulate gravity and orbital mechanics ([video](https://www.youtube.com/watch?v=MTY1Kje0yLg\u0026ab_channel=apbiolghs)). This is reasonable because the dynamics of a ball in a bowl on earth has a homomorphic relationship to orbits--they are both examples of motion in a central potential. This is an interesting mathematical exercise, but it does require a little bit of algebra, so it will be explored in its own small article.\n\n## Homomorphic Relationships Between Emergent Systems\n\nNow that we know what a homomorphism is, and that systems in nature can have homomorphic relationships, let's look back to an image from my [[articles/mission-statement|mission statement]]: Voronoi patterns emerging in seemingly very different situations.\n\n![voronoi-patterns](images/voronoi-patterns.png)\n\nThe *Doing vs. Being* hypothesis is that systems that exhibit the same emergent patterns have a homomorphic relationship to one another. The parts of the system that lead to this pattern are interacting in the same way--they have the same dynamics. \n\nThe Voronoi pattern is meant to be an illustrative example that is easy to understand, since the resemblance between these very different systems is striking, but it is also an isolated and weak case. The pattern only comes about in leaves in the sections between the structural skeleton, or only on the coat of a giraffe. This becomes more interesting and important when we look more directly at systems of competing and cooperating agents with more agency than just expanding until you reach the body of your nearest neighbor.\n\nTo say that these homomorphic relationships exist between emergent systems is to say that what makes an emergent pattern come about is not anything exclusive about the physical system, it is a product of the way the parts interact.\n\nIf we can make little robots that interact in the same way that ants do, would they self organize into a structure that is equivalent to an ant colony?\n\n## Doing vs. Being as a Method of Study\n\nIf this intuition does hold true (which I am still very skeptical of), it would greatly simplify the study of emergent systems. It would mean that any emergent system can be understood simply by finding the right set of rules that describe the way it interacts with its neighbours. If we could figure out the way that neurons interact with one another, and make a bunch of little neuronal robots, the same patterns that emerge in the brain would emerge here--or at least patterns that are described by the same dynamics. We would not need to re-engineer a neuron from scratch, only to understand the types of neurons and the way neurons interact with their neighbors.\n\nThis would effectively reduce the study of [[jargon/Complex System|complex systems]] to the study of the way that agents in populations interact with one another. The study of emergent systems becomes a field that is focused on cybernetics, information theory, and game theory. \n\nNaturally, it would follow that simple instantiations of emergent systems can be used as models or analog computers to simulate the dynamics of much more complex systems that exhibit the same emergent patterns.\n\nNow we have an understanding of what the *Doing vs. Being* argument is claiming. I have some real reservations that come from the way that it is a kind of [[jargon/Functionalism|functionalist perspective]]--similar to the one that inspired the language-based artificial intelligence projects of the 1980s and 1990s. The functionalist approach to artificial intelligence failed in some important ways. I'm excited to explore how these issues may affect the *Doing vs. Being* argument in an upcoming article.","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/articles/mission-statement":{"title":"Mission Statement","content":"Sometimes I wonder what ancient humans thought as they looked at the motions of the stars--as they told their myths of gods and heroes immortalized in constellations.  These people were faced with patterns they could not help but notice, but had no way to describe--and thus all they could do was wonder and create stories. I can imagine the explanation we, today, have for these lights in the sky--that they are burning balls of gas unimaginable distances away--would feel as improbable to them as their description feels to us.\n\nA part of nature that seems to remain as mysterious as it was in ancient times is the realm of [[jargon/emergence|emergence]].\n\n![murmuration of starlings](images/7uUb.gif)\n\nThis is a murmuration of starlings--a flock of little birds. Each starling is entirely unaware of the incredible geometry made by the flock's collective motion. This is emergence--a physical population where the sum is properly greater than the sum of its parts--where the collective behaves in ways that could never be found in the individuals themselves.\n\nEmergent systems have a certain kind of magic to them. Once you are aware of their existence, emergent patterns become ever-present. Every living system, every mountain range--even the cracks that form in the drying of mud--exhibit emergent behavior. Often the same patterns appear in seemingly disparate places. The Voronoi pattern emerges in drying mud, the structure of dragonfly wings and leaves, as well as the camouflage patterns of giraffes.\n\n![voronoi patterns](images/voronoi-patterns.png)\nCan you describe this pattern? The presence of it is very clear, but what actually describes its structure? I find that emergent patterns always seem to be easy to notice, but are quite difficult to describe. They trigger the same pattern recognizing parts of the brain that have us find shapes in the clouds or constellations in the stars, but these patterns are real (or seem to be more real than constellations or cloud-shapes). They captivate wonder, but I don't think studying emergence will deromanticize them in the way that the placement of stars have been (in that any star being near any other in our sky is purely coincidental). \n\nStrangely enough, I was initially exposed to the concept of emergence in a course on philosophy of mind. The sense in which it was presented was:\n\n**\u003e If we assume there is no 'magic' to the mind, then consciousness must emerge from the interaction of neurons, similar to a tornado emerging from the interaction of warm and cool wind currents.**\n\nMost theories of mind in classical philosophy are dependent on some nonmaterial 'soul' or 'spirit' controlling the material body, making this approach seem very pragmatic and grounded in comparison. While the emergentist view of consciousness is based in science, it has been treated a lot like magic--just scientific magic. \n\nI was not so quickly satisfied.\n\nThe validity of the claim is obvious in the same way that the shape of the murmuration is obvious, but just because you can notice the pattern does not mean you can describe it. I began to look to methods of studying these emergent systems. The philosophy was sound, but this theory of emergence seemed to be the place in this argument that has the most room for growth. \n\nThe purpose of this cite is to give me a space to develop projects that have connections to many seemingly disparate parts, consider them on their own, and integrate them.\n\nThese projects often start with questions from Cognitive Science, but have effects that span Physics, Mathematics, Computation, as well as Sociology, Philosophy, and have the visceral beauty that is often only present in Visual Art.","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/articles/on-humility":{"title":"On Humility","content":"\n## Why Did I Make This Site?\n\nIn the spring of 2020, I began to take the question 'how does mind emerge from matter' quite seriously. I was taking Prof. John Vervaeke's Introduction to Cognitive Science, and John is excellent at making convoluted discussions incredibly enticing. The question that was the most salient to me was not one of the necessary and sufficient conditions for mindedness, or the most simple functional form of cognition (although these are very interesting topics), it was *how mind emerges from matter.*\n\n\u003e There is nothing in an ant that describes the colony\n\nThe Cognitive Science program at the University of Toronto (UofT) has a very strong philosophical background and lots of support for students to explore cognition from a philosophical perspective. Professors Jim John and John Vervaeke are very approachable and very kind to their students. There is also significant access to work in artificial intelligence, as UofT is home to Geoffrey Hinton and the titanic Vector Institute. \n\nBut my central interest was not in philosophy or in computation. My interest is motivated by questions in both fields, but I am skeptical that the answers to those questions are likely to be answered be the contemporary approaches to these fields.\n\nMy central interest seemed to be in complex systems--in [[jargon/emergence|emergent systems]]. Systems where the whole is literally greater than the sum of its parts. There is nothing in an ant alone that describes the colony. The colony only exists as it emerges out of the interaction between thousands of ants. Prof. Vervaeke especially was happy to support and encourage this approach to cognitive science papers and more philosophical writing (which I am very grateful for), but I wanted to study emergence from a more empirical perspective. \n\nI put quite a lot of effort into considering what field I should study in alongside cognitive science to enable me to study emergent systems with my career. For the questions that I'm interested in, the most natural choice would be biology, but the truth of [Jsomers' *I Should Have Loved Biology*](https://jsomers.net/i-should-have-loved-biology/) dissuaded that approach.\n\nThe most natural next choice was physics. I tended to be sympathetic to the theories of mind that modelled cognition as a dynamical system--as a series of interacting feedback loops, like [Alicia Juarrero's *Dynamics in Action*](https://direct.mit.edu/books/book/3793/Dynamics-in-ActionIntentional-Behavior-as-a). \n\u003e\"Unlike the processes described by classical thermodynamics, which in their relentless march towards equilibrium forget their past, complex adaptive systems are essentially historical. They embody in their very structure the conditions under which they were created (including the chance events around which each self-organized stage reorganizes). The unrepeatable, random fluctuation or perturbation around which each phase of a sequence of adaptations nucleates leaves its mark on the specific configuration that emerges.\" (Juarrero, 9)\n\nJuarrero's book is really what convinced me to study physics. She was always working towards an Aristotelian metaphor for a kind of cognitive dynamics, but it was one of those theories that seem utterly obvious as the argument progresses--as if it was a tautology from its premises. I was convinced that the connection was more than metaphorical.\n\nTwo years (and some) later, and I have learned many very useful skills and very interesting concepts. My algebra is drastically improved, and I have a whole new lens of beauty for symmetry, for fluid dynamics, and for mathematical modelling in general. As thankful as I am for all of that, emergence has only ever been mentioned in my coursework once. It was with regard to the diffusion of molecules of a solute in some solvent as a collection of random walkers. Often when I mention emergence. I have struggled quite a bit to find supervisors and mentors in emperical complexity science. Of course, it is important to note that the COVID-19 pandemic has also made working with external professors much more difficult.\n\nAfter years of waiting, of learning and preparing to be able to empirically study the dynamics of complex systems, I have decided to make the opportunity to study these systems for myself. The purpose of this site is to house those explorations.\n\n## On Humility\nSince I am performing most of this study on my own, I need to be quite careful with regard to how I speak, as it must not be too authoritative. I will be exploring intuitions quite genuinely. I will likely find my intuitions to be incorrect, and that's a large part of the point. I want this site to be a record of my progression. I hope I often find my intuitions to be wrong, since that means I'm learning.\n\nI think there is something very interesting that comes with developing a concept, realizing its flaws, and iterating further. This iteration is a part of the scientific process that is not often highlighted. Writing about the ways that scientists are grossly wrong 10 times before coming upon a reasonable theory. I can only aim to exemplify and encourage this kind of healthy self-skepticim.\n\nAll of this is to say that I must approach this work with humility. That around all corners, I will likely be the fool. I hope that by the end of a project, I will be less of one. But to do this, I must also write with inegrity--with a clear sense of self-skepticism, absolutely--but I must explore and write about what my genuine intuitions are, and then change my positon in lew of new evidence.\n\nI can only hope that you, my reader, will come along for the ride--and perhaps join the discussion!","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/drafts/Instrumentality-and-the-Extended-Mind":{"title":"Instrumentality and the Extended Mind","content":"\n\u003e This is the outline for a short presentation I gave on September 30, 2022 for PHY340: Issues in Philosophy of Mind at the University of Toronto, taught by Dr. Daniel Munroe\n\nIn [*The Extended Mind*](https://www.jstor.org/stable/3328150) Clarke and Chalmers argue that the tools that augment cognition are not only extensions of the mind, but should be regarded as a part of the mind itself. They use the example of a person playing Tetris™, who uses the the rotation of the shapes not to *position* the tetronimo, but to *determine* whether the tetronimo is compatible with a particular position. The rotation function of the game is *doing cognitive work* for the player. This determinative (i.e. cognitive, non-physical) action is referred to as an *epistemic* action--it aids and augments cognitive processes\". This is in contrast to *pragmatic* actions, which change things in the physical world. Clarke and Chalmers believe that epistemic actions deserve epistemic credit.\n\nThis is their notion of *active externalism*--that external cognitive processes are still cognitive, and should be considered a part of the mind. That a person connected with some external cognitive augmentation becomes a *coupled system* where the cognitive processes of the person become subject to the causes and constraints--the form--of the external system.\n\nMuch of their paper is concerned with address the issue that these coupled systesm may be too easily *decoupled*. Clarke and Chalmers try to resolve this issue by saying that the potential for decoupling does not damage the theory, since these external systems become a part of the mind when they become so reliably coupled to us that they become \"part of the package of cognitive resources that [we] bring to bear on the everyday world\" (11).\n\nClarke and Chalmers further support this claim with the notion of prosthetic memory. They describe Inga, who hears about an exhibition at the Museum of Modern Art (MoMA). Knowing that the MoMA is on 53rd Street, she walks there and visits the museum. They contrast Inga's experience with that of Otto, who has Alzheimer's disease. Otto has learned to use a notebook to act as the part of his cognitive system that stores information like the location of the MoMA. Otto may not know where to go to see this exhibition, but as long as Otto has his notebook to refer back to, he is just as capable of finding the MoMA as Inga is. Otto and his notebook, as a coupled system, are functionally equivalent (at least constrained to the task of navigating to the MoMA) as Inga is.\n\nThis notion of technology acting augmenting the mind, both in the sense of doing cognitive work and in altering the kind of work the mind can do, is all too familiar today. Smartphones act as a map, compass, encyclopedia, calculator, and the most sophisticated notebook coinceivable to Clarke and Chalmers in the 90's. The notion that cognition can be extended is all but irrefutable.\n\nWhere I struggle to follow Clarke and Chalmers is in the distinction between epistemic and pragmatic action. The article itself introduces these terms but sort of glosses over them. The example of epistemic action used is the Tetris player using the \"rotate\" function to see if a tetronimo fits in a particular place rather than performing the rotation mentally. Clarke and Chalmers' example of a pragmatic action is the plugging of a hole in a dam. These examples are not very helpful, so lets consider some new ones.\n\nLet's imagine a mathemetician who writes equations on a chalkboard to be able to track his algebra. Take the chalkboard away, and now the algebra is much more difficult. The chalkboard acts as a kind of prosthetic memory, augmenting the person's ability to do math. There is epistemic work being done by the chalkboard to aid in the doing of math--the act of holding the previous equation is an epistemic action.\n\nNow lets imagine carpenter who uses a hammer and nails and wood to build houses. There is pragmatic work being done by the nails that hold the pieces of the house together. \n\nThe issue that I have is that any pragmatic action is facilitated some epistemic action--the use of tools to do work is afforded by the  way that they extend the mind. The cliche even is 'when all you have is a hammer, everythign is a nail'. Tools change the way we interact with the world. That being said, I struggle to say that tools become a part of the mind when we use them, or use them so regularly that we feel as though we are missing something without them.\n\nIf we take away the hammer away from a craftsman, they cannot do their work. But have we removed a part of their body? I would hesitate to say yes.\n\nLet's go further. Say someone breaks their leg, and while their leg is in a cast, they must use crutches to walk. If we take away their crutches, have we taken away a part of their body? I would still hesitate to say yes.\n\nWhere it really gets muddy is when we speak of a person with a prosthetic limb. If we remove the limb, have we removed part of the body? In this case I would still hesitate to say yes, but I would be more sympathetic with the notion that we have.\n\nIf we say no to these points, how can we rightly say that we remove part of the mathematician's mind if we remove his chalkboard, or remove part of Otto's mind when we remove his notebook?\n\nAs I see it, there are prosthetics and there are augmentations. Prosthetics replace or offload existing cognitive functions. Augmentations afford new cognitive functions. I believe this is true regardless of whether the body or the mind is the thing that is being directly extended. The hammer acts as an extension of the arm when it is held, but it also afford the mind new methods of constructing physical structures. Therefore pragmatic actions are only afforded by the epistemic action of the tools that facilitate them.\n\nThis creates an important dilemmma. If Clarke and Chalmer's stance that epistemic actions deserve epistemic credit is maintained, then any distinction between the mind and world dissolves, and the distinction loses any meaning. On the other hand, the notion of extension cannot be eliminated. \n\nThis is the discussion question. Are prosthetics a part of the self? Where should we draw the line between mind an world? Between body and not-body","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/drafts/computational-irreducibility":{"title":"Computational Irriducibility","content":"Coming soon, for now I defer to [wikipedia](https://en.wikipedia.org/wiki/Chaos_theory)","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/drafts/eidos-and-form":{"title":"The Eidos: Forms and Essences","content":"\nComing soon, for now I defer to [wikipedia](https://en.wikipedia.org/wiki/Theory_of_forms)","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/drafts/emergence-of-objects":{"title":"On the Emergence of Ontologies","content":"\u003eHow does sensory information become abstract thought?\n\nWhat is the physical phenomena that brings about abstract thought?\n\n  \n\nThis appears like an [[jargon/ontology|ontological]] question but it is rather the question of *the emergence of ontology itself*. This makes it not only a philosophical question, but a deep scientific question as well. \n\n  \n\nAs I have explored this question more and more, it has become increasingly clear that it demands a polymathic attitude. The parts are so disparite but so close. This project is meant to be a semi-formal exploration of the project of rigorously studying the emergence of ontologies. \n\n  \n\nI expect I will be wrong left right and center, and that my perspective will change as more work is done and as I am able to do more of my own work. Topics will be written about and returned to. The shape of the project will ebb and flow as we explore this magical snake that eats in own tail, as she crosses through philosophy, to biology, to physics, to computation, to sociology, to the art world and beyond. \n\n  \n\n  \n\nLet's begin this journey with a discussion of the [[maximally-presice-language|bounds of what we can study presicesly]].","lastmodified":"2022-11-09T15:50:36.938826512Z","tags":null},"/drafts/maximally-presice-language":{"title":"Edmund Husserl: What are We Able to Study Rigorously?","content":"\nThere is a powerful irony in our project. It is an attempt to formally study that which has historically eluded formal study. In his very funny [lecture on the legendary 20th century philosopher Edmund Husserl](https://youtu.be/y0sLHfcsPAA), Michael Sugrue describes the difference between the tradition of continental philosophy and that of anglo-americans is that the continental philosophers assume that the self is self-evident. \n\n\u003e...in progress..","lastmodified":"2022-11-09T15:50:36.942826512Z","tags":null},"/drafts/voronoi":{"title":"Building an Intuition: Why do Voronoi Patterns Emerge in so many different places?","content":"","lastmodified":"2022-11-09T15:50:36.942826512Z","tags":null},"/jargon/Complex-System":{"title":"Complex System","content":"A complex system has behavior that is [[jargon/emergence|emergent]] from its constituent parts","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/jargon/Functionalism":{"title":"Functionalism","content":"\nFunctionalism is the notion that the hardware that a mental state runs on is not important, only the role it plays in the larger cognitive system is of consequence.","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/jargon/chaos":{"title":"chaos","content":"Chaos is a high degree of sensitivity to initial conditions.\n\nTo read more see the backlinks or its definition and description in the article on [[computational-irreducibility|computational irreducibility]] ","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/jargon/emergence":{"title":"Emergence","content":"Parts become whole. Emergence is when a population of agents have behaviour not present in their constituent parts.","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/jargon/homomorphism":{"title":"homomorphism","content":"A unidirectional map that maintains the structure of a set of relationships while collapsing all other information into a kernel.\n","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/jargon/self-organization":{"title":"Self-Organization","content":"This one topic is very big and very complicated. I'm very excited to get to it.","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/lists/Music":{"title":"Music","content":"This page contains lists of records that I've listened to, and wanted to track my thoughts of, as well as a second list that holds records that I intend to listen to.\n\n# Have Listened\n- Burial: \n\t- Untrue\n- Tim Hecker:\n\t- Haunt me, Haunt me, Do it Again\n\t- Radio Amor\n\t- Mirages\n\n# To Listen\n- Coil\n\t- Musick to Play in the Dark","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/notes/PHY342-functionalism-paper-planning":{"title":"PHY342 functionalism paper planning","content":"# quotes\n## What is functionalism? \n\n- \"Functionalism is the philosophy of mind based on the distinction that computer science draws between a system's hardware, or physical composition, and its software, or program. The psychology of a system such as a human being, a machine or a disembodied spirit does not depend on the stuff the system is made of (neurons, diodes, or spiritual energy) but on how that stuff is organized. Functionalism does not rule out the possibility, however remote it may be, of mechanical and ethereal systems having mental states and processes.\" (Fodor, 118)\n- \"Functionalism is comitted to defining mental states in terms of their causes and effects\" (Fodor, 122)\n\n\n## According to functionalism, what are beliefs and desires?\n- Andrew in tutorial\n\t- To believe that B = to stand in a certain relationship to a mental symbol in your brain that stands for B.\n\t- Fodor thinks that cognition is the chugging though of symbol operations in the language of thought.\n- \"The traditional view in the philosophy of mind has it that mental states are distinguished by their having what are called either qualitative content or intentional content.\" (Fodor, 122)\n\t- Qualitative content is experiential content. If an image is viewed through a red filter and then a blue filter, something about how that experience *feels* changes.\n\t\t- Functionalism does not account for this, since there is seemingly no consistent causal relationship between input and qualitative experience\n\t\t\t- thus functionalism does not account for consciousness.\n\n## Can functionalism account for the intentional content of beliefs?\n- \"To say that a mental state has intentional content is to say that it has certain semantic properties.\" (Fodor, 122)\n\t-  \"There is at least one kind of thing other than a mental state that has intentional content: a symbol.\" (Fodor, 122)\n- Use Dretske\n\t- \"The idea behind this proscription of intentional ingredients seems to be that since what we are trying to build-a thought-is an intentional product, our recipe cannot use intentional ingredients.\" ... \"This, it seems to me, is a mistake, a mistake that has led some philosophers to despair of ever finding a naturalistic recipe for the mind.\" (Dretske 470)\n\t\t- intentional products are to the mind what copper is to an amplifier\n\t\t\t- \"What we are trying to understand, after all, is not intentionality, per se, but the mind. Thought may be intentional, but that is not the property we are seeking a recipe to understand.\" (Dretske 470)\n\t\t\t- \"Describing what such an instrument indicates is describing it in intensional terms. What one is describing is, therefore, in this sense, an intentional state of the instrument.\" (Dretske, 471)\n\t\t\t- \"The intentional states a compass occupies do not depend on our explanatory purposes, attitudes, or stances.\" (Dretske, 471) \n\t\t\t- \"Intentionality is a much abused word and it means a variety of different things. But one thing it has been used to pick out are states, conditions, and activities having a propositional content the verbal expression of which does not allow the substitution, salvu veritute, of co-refemng expressions.\" (Dretske, 471)\n\t\t- \n\n## Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?\n\n- \"An important tendency in cognitive science is to treat the mind chiefly as a device that manipulates symbols. If a mental process can be functionally defined as an operation on symbols, there is a Turing machine capable of carrying out the computation and a variety of mechanisms for realizing the Turing machine. (Fodor, 120)\"\n- \"Associating the semantic properties of mental states with those of mental symbols is fully compatible with the computer metaphor, because it is natural to think that a computer as a mechanism that manipulates symbols. A computation is a causal chain of computer states, and the links in the chain are operations on semantically interpreted formulas in a machine code. To think of a system (such as the nervous system) as a computer is to raise questions about the nature of the code in which it computes and the semantic properties of the symbols in the code. In fact, the analogy between minds and computers actually implies the postulation of mental symbols. There is no computation without representation\" (Fodor, 122)\n- \"For if one could concoct a recipe for building systems capable of misrepresentation-capable, that is, of saying of something that was not F that it was F-then one would have a recipe for meaning, for constructing structures having a content that was independent of causes in the desired sense\" (Dretske, 472)\n- \"What we would have is a naturalistic recipe for representation, a way of building something that would have, quite apart from its creator’s (or anyone else’s) purposes or thoughts, a propositional content that could be either true or false.\" (Dretske, 474)\n\n# Argument planning\n\n## Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?\n\n- The central claim of the functionalist project is that this is true\n\t- use 1.3.1 or 1.3.2 computation is cognition etc.\n\t- lets buildup Dretske's recipe and see if it is sufficient for creating cognition\n\t\t- \"Our recipe yields a product having the following properties:\n\t\t\t1. The product has a propositional content that represents the world in an aspectual way (as, say, $F$ rather than $G$ even when $F$s are always $G$).\n\t\t\t2. This content can be either true or false.\n\t\t\t3. The product is a “player” in the determination of system output (thus helping to explain system behavior).\n\t\t\t4. The propositional content of this product is the property that explains the product’s role in determining system output. The system not only does what it does because it has this product, but what it is about this product that explains why the system does what it does is its propositional content.\n\t\t\t5. Though the system can behave stupidly, the normal role of this product (the role it will play when it is doing the job for which it was created) will be in the production of intelligent (need and desire satisfaction) behavior.\n\t\tThis, it seems to me, is about all one could ask of a naturalistic recipe for thought. (Dretske, 481)\n\t- Let's step through each of these points and make sure we understand each of them, and see if they come together to answer our question.\n1. The product has a propositional content that represents the world in an aspectual way \n\t- The system we are concerned with must be able to construct mental representations (beliefs and desires) of things in the world in a way that has intentional content.\n2. This content can be either true or false.\n\t- represent information that can be assigned a truth value\n3. The product is a \"player\" in the determination of system output.\n\t- The system must posess some natural function\n4.  The propositional content of this product is the property that explains the product’s role in determining system output.\n\t- this is what Dretske means when he talks about how an altimiter uses air pressure to measure altitude, and thus by putting it in a sealed chamber of low pressure can genuinely misrepresent reality. \n\t- function is the relationship between \n5. The normal role of this product will be in the production of intelligent behavior\n\t- The system must have some rationality\n\nDretske says that any systems that has these things is capable of representation--capable of having beliefs that have the same kind of intentional content as ours, and has constructed that belief for itself.\n\n# Begin Essay:\n\n~~One of the central questions in cognitive science and in the philosophy of mind is the question of whether a non-human machine could be capable of having mental states, of having beliefs. The functionalist approach is that if we understand what functions lead to the having of beliefs, those functions, when instantiated in non-human systems, will still lead to the having of beliefs. Jerry Fodor, one of the titans of functionalism, says the functionalist stance says that \"[t]he psychology of a system such as a human being, a machine or a disembodied spirit does not depend on the stuff the system is made of (neurons, diodes, or spiritual energy) but on how that stuff is organized\" (122). The central claim of the functionalist view is that a non-human machine can have the same kind of beliefs (i.e. mental states) as a human, but they way they define those beliefs is what is most important. In this essay I will argue not that functionalism is wrong, but that the philosophers traditionally associated with the functionalist stance are looking at the wrong level of causality.\n\n## What do the functionalists mean by 'mental state' or 'belief'\n\n~~The form of functionalism of Fodor and Dretske looks at functions on the level of traditional philosophy of mind--which is only natural, as they are (surprisingly) philosphers of mind. And \"The traditional view in the philosophy of mind has it that mental states are distinguished by their having what are called either qualitative content or intentional content\" (Fodor, 118). The qualitative content of a mental state is experiental content, the way that something *feels*. Functionalism is not interested in qualitative content, as it seems to etheral for these strong naturalists--that qualitative content does not have clear causal relationships between mental states. After all \"[f]unctionalism is comitted to defining mental states in terms of their causes and effects\" (Fodor, 122), so they would have little concern of qualitative content. Intentional content on the other hand, that is the domain of functionalism. Intentionalism as Dretske and Fodor speak of it is the having of propositional content. What it means for a statement to have propositional content is that it can be assigned a value of 'true' or 'false' depending on the statement's validity.\n\n~~Dretske takes a thorough approach to describing what functions come together to have a thought or to have a mental state, describing a recipe for thought:\n\"Our recipe yields a product having the following properties:\n\t1. The product has a propositional content that represents the world in an aspectual way (as, say, $F$ rather than $G$ even when $F$s are always $G$).\n\t2. This content can be either true or false.\n\t3. The product is a “player” in the determination of system output (thus helping to explain system behavior).\n\t4. The propositional content of this product is the property that explains the product’s role in determining system output. The system not only does what it does because it has this product, but what it is about this product that explains why the system does what it does is its propositional content.\n\t5. Though the system can behave stupidly, the normal role of this product (the role it will play when it is doing the job for which it was created) will be in the production of intelligent (need and desire satisfaction) behavior.\nThis, it seems to me, is about all one could ask of a naturalistic recipe for thought.\"\" (Dretske, 481)\nWe will build up this argument by discussing each of these points individually, and once we are through, if they all hold valid, we will discuss if they come together to be sufficient for a naturalistic recipe for thought.\n\n## 1. The product has a propositional content that represents the world in an aspectual way\n~~This is Dretske saying that the system we are concerned with must be able to construct mental representations (beliefs and desires) of things in the world in a way that has intentional content. This kind of intentional content is not concerned with motivation or teleology, it is intentional as the having of propositional content. Specifically~~\n\n\n# Essay outline round 2\n## Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?\n\nThesis: There is more to having beliefs than being able to represent information using a structure that requires intentional content.\n\n\"Different persons growing up in the same language are like different bushes\ntrimmed and trained to take the shape of identical elephants. The anatomical details\nof twigs and branches will fulfill the elephantine form differently from bush to bush,\nbut the overall outward results are the same.\" Word and Object (Cambridge: MIT,\n1960), p, 8.\"\n\n## who are the functionalists and what do they think\n- Fodor and Dretske\n- \"Functionalism is comitted to defining mental states in terms of their causes and effects\" (Fodor, 122)\n- From the Functionalist stance, to believe that $B$ is to say that there ","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/notes/PHY460-definitions":{"title":"PHY460 definitions","content":"# Overview\n## The importance of being nonlinear\n### phase space\n- the space where the axes are the parameters of a system of differential equations\n- we are interested in drawing trajectories when given a system\n### trajectory\n- a curve drawn by the solutions (say $(x_1, x_2)$) to system (say $(\\dot{x}_1, \\dot{x}_2)$) in phase space.\n- the curves are the show the dynamics of the system\n### $n$-dimentional system or $n$th-order system\n- a system is $n$-dimensional when it has the coordinates $x_1, x_2, ... , x_n$ and thus has an $n$-dimensional phase space\n- this $n$ determines the dimension of the phase space, and thus also the drawing\n### nonautonomous system\n- A nonautonomous system is a system that is explicitly time dependent\n- makes our equations partial differential equations\n# 1-D Flows\n## Flows on the line\n### one-dimensional or first order system\n- systems where $n=1$, expressed in the form of $\\dot{x} = f(x)$ where $f(x)$ cannot explicitly depend on $t$\n- archetypical system\n### flow\n- if $\\dot{x} \u003e0$ the flow is to the right\n- if $\\dot{x} \u003c 0$ the flow is to the left\n- allows us to characterize the behavior of a system\n### fixed points\n- occur when $\\dot{x} = 0$ \n### stable fixed points\n- occur when $\\ddot{x} \u003e 0$\n- AKA *attractors* or *sinks* \n- with a perturbation to the left, the system gets pushed back to the right, with a perturbation to the left, there is a push back to the left\n- all flows end at a stable fixed point or diverges to infinity\n\n### unstable fixed points\n- occur when $\\ddot{x} \u003c 0$ \n- AKA *repellers* or *sources*\n- any perturbation away from an unstable fixed point sends it to the nearest stable fixed point or out to infinity\n- all flows end at a stable fixed point or diverges to infinity.\n\n## Fixed Points and Stability\n### phase point\n- we take an arbitrary initial condition $x(t_0) = x_0$\n- this point is the phase point\n- we follow the flow from this point on a *phase portrait* to see how the particle behaves as $t \\rightarrow \\infty$\n### trajectory\n- the plot of the evolution of the *phase point* as $t \\rightarrow \\infty$ on the *phase portrait*\n- shows what stable point the *phase point* converges towards, or whether it diverges.\n### phase portrait\n- plot of $(t, x(t))$\n- shows all the qualitatively different trajectories of the system\n### equilibrium\n- represented by fixed points\n- if $x=x^*$ then $x= x^*$ for all time\n- A stable equilibrium are geometrically represented by stable fixed points\n## Population Growth\n### Logistic equation \n- take $N=$ current population, $r=$ growth rate, $K =$ Carrying Capacity  $\\dot{N} = rN \\left( 1 - \\frac{N}{K} \\right)$ And $N = N_0 e^{rt}$\n- models population growth\n- displays fractal bifurcation behavior\n### Carrying Capacity ($K$)\n- if  $N \u003c K$ the population keeps growing with the growth rate $r$\n- if  $N\u003eK$ the population starts decreasing.\n## Linear Stability analysis\n### Linearization about $x^*$ \n- $\\dot{\\eta} \\approx \\eta f'(x)$ where $\\eta = \\frac{d}{dt}(x-x^*) = \\dot{x}$ \n- any perturbation $\\eta(t)$ grows exponentially if $f'(x)\u003e0$ and decays if $f'(x)\u003c0$\n- if $f'(x) = 0$ the test is inconclusive, as this is the higher order terms of the Taylor series are not negligible \n- this allows us to determine the stability of fixed points algebraically\n### characteristic time scale\n- $\\tau=\\frac{1}{|f'(x^*)|}$ \n- $\\tau$ is the time required for $x(t)$ to vary dignificantly in the neighborhood of $x^*$ \n## Existence and uniqueness\n### Existence and Uniqueness theorem\n- we have the initial value problem $$\\dot{x} = f(x), \\ x(o) = x_0$$ If $f(x)$ and $f'(x)$ are continuous on an open interval $R$ of the $x$-axis, and that $x_0 \\in R$. Then the initial value problem has a solution $x(t)$ on some time interval $(-\\tau, \\tau)$ about $t=0$ and this solution is unique.\n- when the solution is not unique, we cannot use the geometric approach to analyzing the behavior of a particle. This is because the particle could move any of the possible ways\n## Impossibility of Oscillations\n### monotonicity\n- $\\dot{x} \\geq 0$ is always true or $\\dot{x}\\leq 0$ is always true\n- when approaching an equilibrium solution, the particle can never overshoot the mark, and damped osciallations can never happen\n### Over Damped Limit\n- for a mechanical system, say a spring that is in a highly viscous fluid, when preturbed, it will not oscillate, rather it will be pulled to an equilibrium point\n- this approach to an equilibrium point follows monotonicity\n## Potentials\n### Potential $V(x)$\n- defined as $$\\dot{x} =f(x) = - \\frac{dV}{dx}$$\n- $V(t)$ decreases along trajectories, and the system always moves towards a lower potential\n### equilibrium point\n- occur when $\\frac{dV}{dx} = 0$ and thus where $V(x)$ remains constant\n- the local minima of $V(x)$ correspond to stable fixed points\n- the local maxima of $V(x)$ correspond to unstable fixed points\n### Double-well potential or Bistable potential\n- a potential with two local minima of $V(x)$\n- a system with 2 stable equilibria\n## Solving Equations on the Computer\n### Euler's Method\n- numerical integration scheme which follows $$ x_{n+1} = x_n + f(x_n)\\Delta t $$\n- must start with some initial condition $x = x_0$ at $t=t_0$ and with a known $f(x)$\n### Error\n- $$ E = |x(t_n)-x_n| $$\n- this is the error in a single timestep.\n### Stepsize $\\Delta t$\n-  the amount of time between each step in a numerical integration\n- smaller $\\Delta t$ means a smaller error\n### Fourth-order Runge-Kutta method\n- thought of as the best compormise between minimizing error as well as compute time\n- First, four numbers are calculated $$ \\begin{split} k_1 \u0026 = f(x_n) \\Delta t \\\\ k_2 \u0026 = f(x_n + \\frac{1}{2} k_1) \\Delta t \\\\ k_3 \u0026 = f(x_n + \\frac{1}{2} k_2) \\Delta t \\\\ k_4 \u0026 = f(x_n +k_3) \\Delta t \\end{split}$$ and then $x_{n+1}$ is given by $$ x_{n+1} = x_n + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4) $$\n### Round-off Error\n- the error inherent to computer calculations, also called *floating point error*\n### Slope Field\n- when solving systems numerically, the first thing we do is plot a slope field on the $(t, x)$ plane. \n- for each point on the plane, $dx/dt$ at that point is computed, and thus makes a field of slopes\n- Once we have ethe slope field, we can draw our trajectories from any initial condition, as governed by our slope field.\n# Bifurcations\n## Introduction to Bifurcations\n### Bifucations\n- changes in position or stability of fixed points\n- qualitative changes in dynamics\n### Bifurcation Points\n- values of the control parameter $r$ where a bifucation occurs\n- acts as a sort of critical point\n## Saddle-Node Bifurcations\n### Saddle-Node Bifurcation\n- Bifurcation where fixed points are created and destroyed\n- prototypically $\\dot{x} = r + x^2$\n- if $r\u003e0$  there are no fixed points\n- if $r = 0$ there is one semi-stable fixed point\n- if $r\u003c0$ there is one stable fixed point and one semi-stable fixed point\n### Bifurcation Diagram\n- plot of $(r, x)$ which shows for what values of $r$ there are bifurcations\n- plotted in parameter space\n- the shape of the plot on a bifurcation diagram allows for the classification of a bifurcaiton\n### Bifurcation curve\n- the plot on a bifurcation diagram is a bifurcation curve, representing the chagne in qualitative behavior in the parameter space\n### Normal Forms\n- protypical forms of a kind of bifurcation\n- representative of all examples of that bifurcation\n## Transcritical Bifurcation\n### Transcritical bifurcation\n- has the normal form $\\dot{x} = rx - x^2$\n- one fixed point always remains, but its stability changes, or rather, undergoes an exchange of stabilities\n### Exchange of Stabilities\n- for when there are two fixed points, and they swap stability after a bifurcation\n- usually goes from ($x_1^*$ stable, $x_0^*$ unstable) $\\rightarrow$ ($x_0^*$ semistable) $\\rightarrow$ ($x_0^*$ stable, $x_2^*$ unstable)\n## Pitchform Bifurcation\n### Symmetry\n- bitchfork bifurcations are found in problems that demonstrate a symmetry\n- fixed points appear in symmetrical pairs\n- if a pillar is to buckle under a certain load, it is as likely to buckle left as to buckle right, and thus has a symmetry\n### Supercritical Pitchform Bifurcation\n- Normal form $\\dot{x} = rx -x^3$ \n- for $r \\leq 0$ there is only one stable point at $x^* = 0$\n- for $r\u003e0$ there are 3 fixed points\n\t- one unstable fixed point at $x^*=0$ \n\t- two stable fixed points at $x^* = \\pm \\sqrt{r}$ \n### Critical Slowing Down\n- in a supercritical fixed point, when $r=0$ the clope of the $(\\dot{x}, x)$ graph is 0 at the fixed point $x^*=0$ \n- this means the solution no longer decays exponentially fast, resulting in a critical slowing down\n### Subcritical pitchfork Bifurcation\n- Normal form $\\dot{x} = rx - x^3$ \n- for $r \\leq 0$ there is only one unstable fixed point at $x^* = 0$\n- for $r\u003e0$ there are 3 stationary points\n\t- one stable fixed point at $x^*=0$ \n\t- two un stable points at $x^* = \\pm \\sqrt{r}$ \n- the actual canonical system of a subcritical pitchfork is $\\dot{x} = rx+x^3-x^5$\n### Symmetry Broken Solutions\n- the solution has less symmetry than the governing equation\n- this points at a fundamental change in dynamics\n### Characteristic Time Scale\n- RETURN TO, YOU DO NOT UNDERSTAND THIS\n### Cusp point\n- when there are two control parameters, $r, \\ h$ \n- occurs when two bifrucation curves meet tangentially on a stability diagram\n- this means two parameters must be tuned\n### Stability Diagram\n- show changes of behavior in parameter space\n### Cusp Catastrophe\n- RETURN TO, YOU DO NOT UNDERSTAND THIS\n# Linear Systems\n## Definitions and Examples\n### Two Dimensional Linear System\n- has the form $$\\begin{split} \\dot{x} \u0026 = ax + by \\\\ \\dot{y} \u0026 = cx + dy\\end{split}$$\n- can be rewrittten in the form of  $\\dot{\\vec{x}} = A \\vec{x}$ where $A = \\begin{pmatrix} a \u0026 b \\\\ c \u0026 d \\end{pmatrix}$ and $\\vec{x} = \\begin{pmatrix} x  \\\\ y \\end{pmatrix}$ \n- linear means that if $\\vec{x}_1$ and $\\vec{x}_2$ are solutions, then any linear combination of these $c_1 \\vec{x}_1 + c_2 \\vec{x}_2$ \n- the solutions to these DE's are trajectories in the phase space\n### Simple harmonic Oscillator\n- has form $m\\ddot{x} + kx = 0$\n- ","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/notes/Papers-reading-list":{"title":"","content":"### Philosophical Papers\n- Nagel - *What is it Like to Be a Bat?* (1974)\n- Haugeland - *Mind Embodied and Embedded* (1998)\n-  Van Gelder - *Dynamics and Cognition* (1996)","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/notes/jc-tattoo-inspo":{"title":"Tattoo Documentation for Jess","content":"\n## Placement\nI'm thinking on my right arm or right leg. I'd be very interested in pieces that work well with motion, preferably spanning two joints.\n\n## Previous tattoos\n\nThese are tattoos that are of the style I'd be interested in (I wanted to embed the images, but Instagram is being painful with how to retrieve links to images)\n\n\u003ehttps://www.instagram.com/p/CZhlsnpO7I5/\n\u003e\tI love the way leaves fold over each other here\n\u003ehttps://www.instagram.com/p/CXL7ZtmrIDw/\n\u003e\tyou have a lot of pieces like this, that work very well in motion, which I am also very interested in. More examples here. \n\u003e\t1. https://www.instagram.com/p/CflxGjCuWww/\n\u003e\t2. https://www.instagram.com/p/Ce1trp9vlLg/\n\u003e\t3. https://www.instagram.com/p/CdnvNfAuGBJ/\n\u003e\t4. https://www.instagram.com/p/CVk7RNerGs2/\n\u003ehttps://www.instagram.com/p/CVDhmx-grT8/\n\u003e\tthis piece has a lot of fine detail work, which I'm tempted by, since it gives room for some of the self-organizing patters discussed below\n\u003ehttps://www.instagram.com/p/Cj5sjIYu5cT/\n\tI also absolutely love when the leaves twist around themselves and make helicies, like this example\n\n## Forms from Complexity: Self-Similarity and Self-Organization\n\nOne hallmark of self-organized systems, or moments of spontaneous order is self-similarity. Essentially, this means that there is some fractal behavior present. An example of this is the mandelbrot set.\n\n![Feigenbaumzoom.gif](images/Feigenbaumzoom.gif)\n\nYou can see that as the image zooms in, the structure of the pattern is maintained. Similar happens with a Koch Curve. \n\n![KochSnowGif16_800x500_2.gif](images/KochSnowGif16_800x500_2.gif)\n\nMany living systems exhibit self-similar behavior. This is a romenesco cauliflower\n\n![roma-cal.png](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Flickr_-_cyclonebill_-_Romanesco.jpg/1600px-Flickr_-_cyclonebill_-_Romanesco.jpg)\n\nNotice that the small knobs exhibit the same behavior as the whole head of the cauliflower, and the knobs on each knob follow that same pattern as well. \n\nAnother famous example of self-similarity in plants is in ferns. This is a mathematical diagram called the Barnsley Fern.\n\n![Fractal_fern_explained.png](images/Fractal_fern_explained.png)\n\nReal ferns are self-similar in slightly less perfect ways, but still self-similar. Here are some examples\n\n![fiddlehead-2.jpeg](images/fiddlehead-2.jpeg) \n\nNot only the central branch coils up, but also the smaller 'leaf' branches as well. This image is not particularly fractal, but demonstrates this leaf-coiling well.\n\n![fiddlehead-3.jpeg](images/fiddlehead-3.jpeg)\n\nHere is a photo of a fern that I took in pacific spirit park, near my home in Vancouver. Notice how the leaves are little ferns in themselves.\n\n![van-fern.png](images/van-fern.png)\n\nOr, also in my neighborhood at home, the branches of trees exhibit fractal branching patterns\n\n![van-snow-branches.png](images/van-snow-branches.png)\n\nLichens also exhibit some of the most incredible fractal behavior. This is another example found on the underside of a tree in Vancouver.\n\n![van-lichen.png](images/van-lichen.png)\n\nOr, in much more incredible fashion, these are lichens from Meare's island close to Tofino, one of the last untouched old growth forests in Canada. These lichens are likely hundreds of years old\n\n![meares-lichen.png](images/meares-lichen.png)\n\nAt this level, they really start to take alien form, like plants from a different planet.\n\nI'm absolutely not expecting all (or any) of these fractal points to end up in my tattoo. If it could, I think it would be super cool, but I also imagine it would take a lot of work. I got a little carried away once I started looking into it, but I think it super cool and pretty.\n","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null},"/notes/levin-reading-notes":{"title":"levin-reading-notes","content":"# The Biophysics of Regenerative Repair Suggests New Perspectives on Biological Causation (M. Levin, 2020)\n- Anatomical “macrostates” can be implemented by a wide range of molecular and cellular arrangements and even time-series trajectories that all implement the same large-scale outcomes.[5–7]\n- Artificial Life in silico and in vitro[15]\n- It is impossible here to do justice to the rich literature on causation in biology, and philosophically inclined readers are invited to delve deeper.[11,14,21–35]\n- [relevant causation is] a mechanism or event is the cause of some outcome when it provides the most efficient way for experimenters, or the biological system itself, to induce it to occur, prevent it from occurring, or modify how it occurs.\n- There has been a rich debate in philosophy and science about the nature of causation,[6,8,12,13]\n- Any set of components will give rise to some emergent electrical pattern (akin to turing pattern self-organization from a homogenous substrate[80]), but if the components and their connectivity map has been subject to selection, it will give rise to electrical dynamics that are robust, process inputs into useful outputs, and in some cases can even implement memory (subsequent activity is modified by past experiences or input signals)\n- genetic descriptions belief that morphodynamics are based on linear gene expression\n\t- this should lead to errors propogating downstream\n- these errors in fact reduce in frequency downstream\n\t- the model must be incomplete\n- behaviors are not based on states of local environment\n\t- if you graft the tail of salamander onto the place where a tail should be, it slowly turns into an arm\n- picasso tadpoles\n\t- even if you put all the parts of a tadpole in the wrong place, they take unorthodox paths towards the correct position as they metamorphasize into frogs\n\t\t- morphology is not a linear execution of experiements, but rather governed by attractors seeking stable states\n\t\t\t- via dynamical systems theory\n- [A teleological] perspective offers an important and practical strategy for bioengineers: re-writing the stored setpoint (and letting un-modified cells build to that new specification), instead of attempting to micromanage (re-wire) individual cell interaction rules, hoping emergence of desired largescale outcomes.\n\n## A Scalable Pipeline for Designing Reconfigurable Organisms\n- how are you giving the cells instructions???\n\n## TAME\n### Introduction\n- all cognitive agents are collective intelligences\n- cognition is not only self reflexive advanced cognition or metacognition, but also recognizes adaptive responsiveness and actions of different levels of sophistication\n\t- creates a continuum of cognitive sophistication, giving rise to an *axis of persuadability* \n\t- #### What is the axis of persuadability? What is its relationship with the intentional stance?\n\t\t- \"Persuadabilty refers to the type of conceptual and practical tools that are optimal to rationally modify a given system’s behavior.\" (4)\n### Cognition: Changing the Subject\n- \"The embodied nature of cognition means that mental Selves are dependent on a highly plastic material substrate which changes not only on evolutionary time scales but also during the lifetime of the agent itself.\" (5)\n\t- the Self is subject to significant change in real time\n\t- this can be slow, more ordinary change, or can be a more radical change in material (e.g. memories can be maintained from before and after the liquification of the caterpillar brain)\n- what are invariants that enable a self to persist?\n\t- Memories?\n\t\t- but memories can be transferred between individuals by way of brain tissue implants or other molecular engrams\n\t\t- memories are really communication between past and future Selves--biological bodies are made up of cells that die, are born and significantly rearrange, thus questions of how cognition can persist are questions of swarm dynamics rather than purely stable structures or programs\n\t- #### check back on [42; 43; 44; 45] for what sensory substitution is\n- Hybrots (hybrid robots and animals) plates of neurons can learn how to fly a flight simulator [53; 54; 55]\n### Philosophical Foundations of an Approach to Diverse Intelligences\n- a commitment to gradualism with respect to all important cognition-related properties\n\t- there is no true clear line that demarcates 'true cognition' from 'just physics'\n- example of issue: bilogically evolved froms have intrinsic motivation while software AI agents are only faking it *via functional performance*\n\t- but do fish or single cells or mitochondria *actually care*? \n\t- if we make hybrids of robots and cells how many cells does the system need before that living system's 'true' cognition extends to the whole system?\n- There is no privileged material substrate for Selves\n\t- ","lastmodified":"2022-11-09T15:50:37.238826458Z","tags":null}}