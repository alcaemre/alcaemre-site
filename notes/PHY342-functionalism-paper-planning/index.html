<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="quotes What is functionalism?  &ldquo;Functionalism is the philosophy of mind based on the distinction that computer science draws between a system&rsquo;s hardware, or physical composition, and its software, or program."><title>PHY342 functionalism paper planning</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://alcaemre.com//icon.png><link href=https://alcaemre.com/styles.066590d157012a272b5b56dfc71c4e79.min.css rel=stylesheet><link href=https://alcaemre.com/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://alcaemre.com/js/darkmode.eee95271d06c6ebc1921fbd6ce6591eb.min.js></script>
<script src=https://alcaemre.com/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://alcaemre.com/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://alcaemre.com/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://alcaemre.com/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://alcaemre.com/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://alcaemre.com/",fetchData=Promise.all([fetch("https://alcaemre.com/indices/linkIndex.a2171829f4e0198759a22f803a7b05b7.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://alcaemre.com/indices/contentIndex.bf7175cc8405df48656cd305c99bc84c.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://alcaemre.com",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://alcaemre.com",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/alcaemre.com\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://alcaemre.com/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://alcaemre.com/>alcaemre</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>PHY342 functionalism paper planning</h1><p class=meta>Last updated
Oct 28, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/notes/PHY342%20functionalism%20paper%20planning.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://alcaemre.com/tags/PHY342/>Phy342</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#what-is-functionalism>What is functionalism?</a></li><li><a href=#according-to-functionalism-what-are-beliefs-and-desires>According to functionalism, what are beliefs and desires?</a></li><li><a href=#can-functionalism-account-for-the-intentional-content-of-beliefs>Can functionalism account for the intentional content of beliefs?</a></li><li><a href=#is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs>Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?</a></li></ol><ol><li><a href=#is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs-1>Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?</a></li></ol><ol><li><a href=#what-do-the-functionalists-mean-by-mental-state-or-belief>What do the functionalists mean by &lsquo;mental state&rsquo; or &lsquo;belief&rsquo;</a></li><li><a href=#1-the-product-has-a-propositional-content-that-represents-the-world-in-an-aspectual-way>1. The product has a propositional content that represents the world in an aspectual way</a></li></ol><ol><li><a href=#is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs-2>Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?</a></li><li><a href=#who-are-the-functionalists-and-what-do-they-think>who are the functionalists and what do they think</a></li></ol></nav></details></aside><a href=#quotes><h1 id=quotes><span class=hanchor arialabel=Anchor># </span>quotes</h1></a><a href=#what-is-functionalism><h2 id=what-is-functionalism><span class=hanchor arialabel=Anchor># </span>What is functionalism?</h2></a><ul><li>&ldquo;Functionalism is the philosophy of mind based on the distinction that computer science draws between a system&rsquo;s hardware, or physical composition, and its software, or program. The psychology of a system such as a human being, a machine or a disembodied spirit does not depend on the stuff the system is made of (neurons, diodes, or spiritual energy) but on how that stuff is organized. Functionalism does not rule out the possibility, however remote it may be, of mechanical and ethereal systems having mental states and processes.&rdquo; (Fodor, 118)</li><li>&ldquo;Functionalism is comitted to defining mental states in terms of their causes and effects&rdquo; (Fodor, 122)</li></ul><a href=#according-to-functionalism-what-are-beliefs-and-desires><h2 id=according-to-functionalism-what-are-beliefs-and-desires><span class=hanchor arialabel=Anchor># </span>According to functionalism, what are beliefs and desires?</h2></a><ul><li>Andrew in tutorial<ul><li>To believe that B = to stand in a certain relationship to a mental symbol in your brain that stands for B.</li><li>Fodor thinks that cognition is the chugging though of symbol operations in the language of thought.</li></ul></li><li>&ldquo;The traditional view in the philosophy of mind has it that mental states are distinguished by their having what are called either qualitative content or intentional content.&rdquo; (Fodor, 122)<ul><li>Qualitative content is experiential content. If an image is viewed through a red filter and then a blue filter, something about how that experience <em>feels</em> changes.<ul><li>Functionalism does not account for this, since there is seemingly no consistent causal relationship between input and qualitative experience<ul><li>thus functionalism does not account for consciousness.</li></ul></li></ul></li></ul></li></ul><a href=#can-functionalism-account-for-the-intentional-content-of-beliefs><h2 id=can-functionalism-account-for-the-intentional-content-of-beliefs><span class=hanchor arialabel=Anchor># </span>Can functionalism account for the intentional content of beliefs?</h2></a><ul><li>&ldquo;To say that a mental state has intentional content is to say that it has certain semantic properties.&rdquo; (Fodor, 122)<ul><li>&ldquo;There is at least one kind of thing other than a mental state that has intentional content: a symbol.&rdquo; (Fodor, 122)</li></ul></li><li>Use Dretske<ul><li>&ldquo;The idea behind this proscription of intentional ingredients seems to be that since what we are trying to build-a thought-is an intentional product, our recipe cannot use intentional ingredients.&rdquo; &mldr; &ldquo;This, it seems to me, is a mistake, a mistake that has led some philosophers to despair of ever finding a naturalistic recipe for the mind.&rdquo; (Dretske 470)<ul><li>intentional products are to the mind what copper is to an amplifier<ul><li>&ldquo;What we are trying to understand, after all, is not intentionality, per se, but the mind. Thought may be intentional, but that is not the property we are seeking a recipe to understand.&rdquo; (Dretske 470)</li><li>&ldquo;Describing what such an instrument indicates is describing it in intensional terms. What one is describing is, therefore, in this sense, an intentional state of the instrument.&rdquo; (Dretske, 471)</li><li>&ldquo;The intentional states a compass occupies do not depend on our explanatory purposes, attitudes, or stances.&rdquo; (Dretske, 471)</li><li>&ldquo;Intentionality is a much abused word and it means a variety of different things. But one thing it has been used to pick out are states, conditions, and activities having a propositional content the verbal expression of which does not allow the substitution, salvu veritute, of co-refemng expressions.&rdquo; (Dretske, 471)</li></ul></li><li></li></ul></li></ul></li></ul><a href=#is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs><h2 id=is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs><span class=hanchor arialabel=Anchor># </span>Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?</h2></a><ul><li>&ldquo;An important tendency in cognitive science is to treat the mind chiefly as a device that manipulates symbols. If a mental process can be functionally defined as an operation on symbols, there is a Turing machine capable of carrying out the computation and a variety of mechanisms for realizing the Turing machine. (Fodor, 120)&rdquo;</li><li>&ldquo;Associating the semantic properties of mental states with those of mental symbols is fully compatible with the computer metaphor, because it is natural to think that a computer as a mechanism that manipulates symbols. A computation is a causal chain of computer states, and the links in the chain are operations on semantically interpreted formulas in a machine code. To think of a system (such as the nervous system) as a computer is to raise questions about the nature of the code in which it computes and the semantic properties of the symbols in the code. In fact, the analogy between minds and computers actually implies the postulation of mental symbols. There is no computation without representation&rdquo; (Fodor, 122)</li><li>&ldquo;For if one could concoct a recipe for building systems capable of misrepresentation-capable, that is, of saying of something that was not F that it was F-then one would have a recipe for meaning, for constructing structures having a content that was independent of causes in the desired sense&rdquo; (Dretske, 472)</li><li>&ldquo;What we would have is a naturalistic recipe for representation, a way of building something that would have, quite apart from its creator’s (or anyone else’s) purposes or thoughts, a propositional content that could be either true or false.&rdquo; (Dretske, 474)</li></ul><a href=#argument-planning><h1 id=argument-planning><span class=hanchor arialabel=Anchor># </span>Argument planning</h1></a><a href=#is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs-1><h2 id=is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs-1><span class=hanchor arialabel=Anchor># </span>Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?</h2></a><ul><li>The central claim of the functionalist project is that this is true<ul><li>use 1.3.1 or 1.3.2 computation is cognition etc.</li><li>lets buildup Dretske&rsquo;s recipe and see if it is sufficient for creating cognition<ul><li>&ldquo;Our recipe yields a product having the following properties:<ol><li>The product has a propositional content that represents the world in an aspectual way (as, say, $F$ rather than $G$ even when $F$s are always $G$).</li><li>This content can be either true or false.</li><li>The product is a “player” in the determination of system output (thus helping to explain system behavior).</li><li>The propositional content of this product is the property that explains the product’s role in determining system output. The system not only does what it does because it has this product, but what it is about this product that explains why the system does what it does is its propositional content.</li><li>Though the system can behave stupidly, the normal role of this product (the role it will play when it is doing the job for which it was created) will be in the production of intelligent (need and desire satisfaction) behavior.
This, it seems to me, is about all one could ask of a naturalistic recipe for thought. (Dretske, 481)</li></ol></li></ul></li><li>Let&rsquo;s step through each of these points and make sure we understand each of them, and see if they come together to answer our question.</li></ul></li></ul><ol><li>The product has a propositional content that represents the world in an aspectual way<ul><li>The system we are concerned with must be able to construct mental representations (beliefs and desires) of things in the world in a way that has intentional content.</li></ul></li><li>This content can be either true or false.<ul><li>represent information that can be assigned a truth value</li></ul></li><li>The product is a &ldquo;player&rdquo; in the determination of system output.<ul><li>The system must posess some natural function</li></ul></li><li>The propositional content of this product is the property that explains the product’s role in determining system output.<ul><li>this is what Dretske means when he talks about how an altimiter uses air pressure to measure altitude, and thus by putting it in a sealed chamber of low pressure can genuinely misrepresent reality.</li><li>function is the relationship between</li></ul></li><li>The normal role of this product will be in the production of intelligent behavior<ul><li>The system must have some rationality</li></ul></li></ol><p>Dretske says that any systems that has these things is capable of representation&ndash;capable of having beliefs that have the same kind of intentional content as ours, and has constructed that belief for itself.</p><a href=#begin-essay><h1 id=begin-essay><span class=hanchor arialabel=Anchor># </span>Begin Essay:</h1></a><p>~~One of the central questions in cognitive science and in the philosophy of mind is the question of whether a non-human machine could be capable of having mental states, of having beliefs. The functionalist approach is that if we understand what functions lead to the having of beliefs, those functions, when instantiated in non-human systems, will still lead to the having of beliefs. Jerry Fodor, one of the titans of functionalism, says the functionalist stance says that &ldquo;[t]he psychology of a system such as a human being, a machine or a disembodied spirit does not depend on the stuff the system is made of (neurons, diodes, or spiritual energy) but on how that stuff is organized&rdquo; (122). The central claim of the functionalist view is that a non-human machine can have the same kind of beliefs (i.e. mental states) as a human, but they way they define those beliefs is what is most important. In this essay I will argue not that functionalism is wrong, but that the philosophers traditionally associated with the functionalist stance are looking at the wrong level of causality.</p><a href=#what-do-the-functionalists-mean-by-mental-state-or-belief><h2 id=what-do-the-functionalists-mean-by-mental-state-or-belief><span class=hanchor arialabel=Anchor># </span>What do the functionalists mean by &lsquo;mental state&rsquo; or &lsquo;belief&rsquo;</h2></a><p>~~The form of functionalism of Fodor and Dretske looks at functions on the level of traditional philosophy of mind&ndash;which is only natural, as they are (surprisingly) philosphers of mind. And &ldquo;The traditional view in the philosophy of mind has it that mental states are distinguished by their having what are called either qualitative content or intentional content&rdquo; (Fodor, 118). The qualitative content of a mental state is experiental content, the way that something <em>feels</em>. Functionalism is not interested in qualitative content, as it seems to etheral for these strong naturalists&ndash;that qualitative content does not have clear causal relationships between mental states. After all &ldquo;[f]unctionalism is comitted to defining mental states in terms of their causes and effects&rdquo; (Fodor, 122), so they would have little concern of qualitative content. Intentional content on the other hand, that is the domain of functionalism. Intentionalism as Dretske and Fodor speak of it is the having of propositional content. What it means for a statement to have propositional content is that it can be assigned a value of &rsquo;true&rsquo; or &lsquo;false&rsquo; depending on the statement&rsquo;s validity.</p><p>~~Dretske takes a thorough approach to describing what functions come together to have a thought or to have a mental state, describing a recipe for thought:
&ldquo;Our recipe yields a product having the following properties:
1. The product has a propositional content that represents the world in an aspectual way (as, say, $F$ rather than $G$ even when $F$s are always $G$).
2. This content can be either true or false.
3. The product is a “player” in the determination of system output (thus helping to explain system behavior).
4. The propositional content of this product is the property that explains the product’s role in determining system output. The system not only does what it does because it has this product, but what it is about this product that explains why the system does what it does is its propositional content.
5. Though the system can behave stupidly, the normal role of this product (the role it will play when it is doing the job for which it was created) will be in the production of intelligent (need and desire satisfaction) behavior.
This, it seems to me, is about all one could ask of a naturalistic recipe for thought.&rdquo;&rdquo; (Dretske, 481)
We will build up this argument by discussing each of these points individually, and once we are through, if they all hold valid, we will discuss if they come together to be sufficient for a naturalistic recipe for thought.</p><a href=#1-the-product-has-a-propositional-content-that-represents-the-world-in-an-aspectual-way><h2 id=1-the-product-has-a-propositional-content-that-represents-the-world-in-an-aspectual-way><span class=hanchor arialabel=Anchor># </span>1. The product has a propositional content that represents the world in an aspectual way</h2></a><p><del>This is Dretske saying that the system we are concerned with must be able to construct mental representations (beliefs and desires) of things in the world in a way that has intentional content. This kind of intentional content is not concerned with motivation or teleology, it is intentional as the having of propositional content. Specifically</del></p><a href=#essay-outline-round-2><h1 id=essay-outline-round-2><span class=hanchor arialabel=Anchor># </span>Essay outline round 2</h1></a><a href=#is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs-2><h2 id=is-it-possible-for-a-non-human-machine-to-believe-the-very-same-thing-as-you-ie-to-have-a-belief-that-has-the-same-intentional-content-as-one-of-your-beliefs-2><span class=hanchor arialabel=Anchor># </span>Is it possible for a non-human machine to believe the very same thing as you. i.e. to have a belief that has the same intentional content as one of your beliefs?</h2></a><p>Thesis: There is more to having beliefs than being able to represent information using a structure that requires intentional content.</p><p>&ldquo;Different persons growing up in the same language are like different bushes
trimmed and trained to take the shape of identical elephants. The anatomical details
of twigs and branches will fulfill the elephantine form differently from bush to bush,
but the overall outward results are the same.&rdquo; Word and Object (Cambridge: MIT,
1960), p, 8."</p><a href=#who-are-the-functionalists-and-what-do-they-think><h2 id=who-are-the-functionalists-and-what-do-they-think><span class=hanchor arialabel=Anchor># </span>who are the functionalists and what do they think</h2></a><ul><li>Fodor and Dretske</li><li>&ldquo;Functionalism is comitted to defining mental states in terms of their causes and effects&rdquo; (Fodor, 122)</li><li>From the Functionalist stance, to believe that $B$ is to say that there</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://alcaemre.com/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Emre Alca using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://alcaemre.com/>Home</a></li><li><a href=mailto:emre.alca@icloud.com>Contact</a></li></ul></footer></div></div></body></html>