---
title: "What You Must Keep, What You May Discard"
tags:
- "PHL342"
- "doing-vs-being"
---

## Introduction

Developing a theory, of any kind, in any field, is a non-trivial task. Developing a theory of mind is the most abstract of theoretical realms. I think that the abstractness of the mind demands a certain level of theoretical care. Physicists talk about a theory as a *[[jargon/homomorphism|homomorphism]]*--as a unidirectional map from the real world to a model one. In this model world, all the relevant relationships are maintained, and all other information is collapsed into a *kernel*. Some physicists say that the real work of being a theorist lies in the creation of the kernel. The larger your kernel is, the more elegant the theory, but also the more likely that some important relationship is missing. If the kernel is too large, the theory is clumsy and inefficient. Many physicists say that the true skill of a theorist is the ability to discern what you must keep and what you may discard. 

This paper is concerned with the question "is it possible for a non-human machine to believe the very same thing as you (i.e. to have a belief that has the same intentional content as one of your beliefs)?" Specifically, the perspective of the [[jargon/Functionalism|Functionalists]], the Eliminative Materialists, and later in the paper, Daniel Dennett (who is particularly difficult to label--and probably quite proud of it). I believe the disagreements between these stances arises from their different kernels.

## The Functionalist Stance
The Functionalists (with a capital 'F' as I may yet be a functionalist, but I know I am not a Functionalist) are interested in a computational metaphor for cognition. I think Fodor is quite clear himself when he says:

>"An important tendency in cognitive science is to treat the mind chiefly as a device that manipulates symbols. If a mental process can be functionally defined as an operation on symbols, there is a Turing machine capable of carrying out the computation and a variety of mechanisms for realizing the Turing machine. (Fodor, 120)"

This shows Fodor's true mission. The mental states can have intentional content or qualitative content. Qualitative content encompasses feelings (often referred to as 'what it is like'-ness) associated with an experience. Intentional content is propositional content--something that can be assigned a truth-value. Fodor is not concerned with qualitative content. He believes there are no causal relationships between the qualitative content of mental states. On the other hand, beliefs with intentional content have clear causal relationships. They have implications, they can build into larger arguments, they are the foundation of formal logic. Fodor thinks that the only structure we need to be interested in is the one that describes the relationships between the intentional content of beliefs--in the language of thought.

Dretske is a Functionalist who treats the argument with a little more sensitivity. He discusses what it means for a system to have intentional beliefs, and especially what it means for a system to have intentional beliefs of its own. Dretske uses the example of a compass. He says that a compass always represents some propositional content, even when there is no person looking at it. "The intentional states a compass occupies do not depend on our explanatory purposes, attitudes, or stances" (Dretske, 471). Even when the compass is stowed away in a pocket, it is still pointing north. To build up to the level of a system that not only represents intentional content, but can create representations with the same kind of intentional content that our representations have, Dretske builds up to a more thorough recipe:

> "Our recipe yields a product having the following properties:
			1. The product has a propositional content that represents the world in an aspectual way (as, say, $F$ rather than $G$ even when $F$s are always $G$).
			2. This content can be either true or false.
			3. The product is a “player” in the determination of system output (thus helping to explain system behavior).
			4. The propositional content of this product is the property that explains the product’s role in determining system output. The system not only does what it does because it has this product, but what it is about this product that explains why the system does what it does is its propositional content.
			5. Though the system can behave stupidly, the normal role of this product (the role it will play when it is doing the job for which it was created) will be in the production of intelligent (need and desire satisfaction) behavior.
	This, it seems to me, is about all one could ask of a naturalistic recipe for thought." (Dretske, 481)

Parts 1. and 2. are building up that this 'product' must be able to represent intentional content according to its most specific aspect (e.g. jersey cows are a category even though all jersey cows are cows). Part 3. speaks to how a system must have some natural function which informs its structure (e.g. the structure of the heart is informed by its function of pumping blood). Part 4. speaks to how the system must be able to genuinely misrepresent things in the world (e.g. an altimeter put in a vacuum chamber will think it is very high in the atmosphere, even if it is at sea level). Finally, 5. posits that a mental state must be able to interact with other mental states--to be capable of that Fodorian symbol operation. I think that Dretske has laid out a very clear account of what it is for a belief to have intentional content, but I do not see how these parts can be put together into something that has the kind of beliefs that humans do.

## Two Bushes Trimmed to the Shape of Two Elephants
In Dennett's *Real Patterns*, there is a quote from Quine, that is quite tragically left to a footnote. Quine says:

>"Different persons growing up in the same language are like different bushes trimmed and trained to take the shape of identical elephants. The anatomical details of twigs and branches will fulfill the elephantine form differently from bush to bush, but the overall outward results are the same." (Quine, 1960, 8)

This quote sums up my problem with both functionalism and Eliminative materialism. To look only at the shape of the bush from the outside misses the very important internal structure of branches and roots. We are not Laplace's demon, we cannot look only at the structure of the branches and roots and determine the overall shape. The kernel is too small or too big. The Eliminative Materialists keep too much, and the Functionalists keep too little.

Dennett thinks we need to be somewhere closer to the middle of this debate. He believes that mental states cannot be dispensed of, because we are looking to make a system that has mental states. Dennett proposes theory on the level of *intentions* (as in the seeking of goals, not the having of propositional content) as a reliable way of predicting behavior. This is because it allows us to detect the important relationships in the most efficient way--to develop a kind of 'folk psychology'. The intentional stance keeps what is necessary and discards the rest. 
Dennett is also quite careful to say that while this is the right level to talk about the mind--to notice the interesting patters, it is not the level where the thoughts are happening. He says:

>"The process that produces the data of folk psychology, we claim, is one in which the multidimensional complexities of the underlying processes are projected *through linguistic behavior*, which creates an appearance of definiteness and precision, thanks to the discreteness of words" (Dennett, 45)

Dennett is more sympathetic to the idea that mental states are not explicitly things in the head, but something closer to "indirect 'measurements' of a reality diffused in the behavioral dispositions of the brain (and body)" (Dennett, 45). This is an idea that is originally from Churchland, and Dennett co-opts it quite carefully--he maintains that while the interesting action is not happening on the level of some 'language of thought', there is some pattern above the level of just sets of neurons firing. This is what he means when he says "If the "pattern" is scarcely an improvement over the bit map, talk of eliminative materialism will fall on deaf ears—just as it does when radical eliminativists urge us to abandon our ontological commitments to tables and chairs." (Dennett, 51). 

## How Do We Find the Right Level For Analysis?

I am quite sympathetic to Dennett's definition of prescription: a system has mental states when it can be best and most robustly described from the intentional stance--when it can be attributed beliefs and desires that reliably predict its behavior. But I'm still not quite satisfied. I am a kind of functionalist, I do believe that a non-human machine can have the same kind of mental states that we do (and not just beliefs with intentional content, but qualitative content as well). Animals are the first example--they clearly can have mental states even though they are non-human.

![murmuration of starlings](images/7uUb.gif)
> Fig.1 A murmuration of starlings

Before returning to the discussion to the level of minds, I would like to point at a murmuration of starlings. I believe that if we make little robots that have similar physical constraints and similar goals (be that differential attractors or some other mechanism) to real starlings, they will, in some important sense, become starlings--and thus will make murmurations of their own. We do not need to know all the mechanics of all the birds, just the rules that describe their interactions with one another. When working with these [[jargon/emergence|emergent patters]], identifying the right level of interaction is very nontrivial. For the mind, the level of the cell, or the signal seems quite natural to me, but it still is at great risk of falling prey to Churchland's issue of not doing enough compression. And still, looking at too high of a level, something like the level of propositional attitudes seems like too great a leap. Dennett's notion of a multileveled ontology, to make changes at the low level to look at outcomes at the high level, seems quite promising to me.

## References
1. Churchland, PM. (1981). Eliminative Materialism And The Propositional Attitudes. The Journal of Philosophy.
2. Dennett, DC. (1991). Real Patterns. The Journal of Philosophy.  
3. Dretske, F. (1994). If You Can’t Make One, You Don’t Know How It Works. Midwest Studies in Philosophy.  
4. Fodor, J. (1980). The Mind-Body Problem. Scientific American. 
5. Quine, WVO. (1960). Word and Object. MIT Press.